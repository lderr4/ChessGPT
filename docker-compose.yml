version: "3.8"

services:
  # ... Postgres and Redis remain exactly the same ...
  postgres:
    image: postgres:15-alpine
    container_name: chess_analytics_db
    environment:
      POSTGRES_DB: chess_analytics
      POSTGRES_USER: chess_user
      POSTGRES_PASSWORD: chess_password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U chess_user -d chess_analytics"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: chess_analytics_redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # 1. THE API SERVICE (Renamed from 'backend')
  # Role: HTTP Server, Producer of Tasks, I/O Bound
  api:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: chess_analytics_api
    environment:
      # ... All your env vars remain the same ...
      DATABASE_URL: postgresql://chess_user:chess_password@postgres:5432/chess_analytics
      REDIS_URL: redis://redis:6379/0
      WATCHFILES_FORCE_POLLING: true
      # ...
    ports:
      - "8000:8000" # Only the API needs to be exposed
    volumes:
      - ./backend:/app
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    # Command stays as the web server start
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  # 2. THE WORKER SERVICE (New)
  # Role: Consumer of Tasks, CPU Bound (Stockfish)
  stockfish_worker:
    build:
      context: ./backend # Re-use the same code/image for now
      dockerfile: Dockerfile
    # No container_name here! If you set a static name, you cannot scale it (you'd get name conflicts).
    environment:
      DATABASE_URL: postgresql://chess_user:chess_password@postgres:5432/chess_analytics
      REDIS_URL: redis://redis:6379/0
      STOCKFISH_PATH: /usr/games/stockfish
      # OLLAMA CONFIG (Workers need this for the AI Coach logic)
      ENABLE_COACH: false
      COACH_PROVIDER: ollama
      OLLAMA_BASE_URL: http://host.docker.internal:11434
      OLLAMA_MODEL: llama3.1
    volumes:
      - ./backend:/app # Useful for dev so you don't have to rebuild to change worker logic
    depends_on:
      api:
        condition: service_started # Wait for API just to be safe, though technically only needs Redis
      redis:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    # THE CRITICAL DIFFERENCE: This command starts the queue listener, NOT the web server.
    # We will implement "app.worker" in the next layer.
    command: watchfiles --filter python "celery -A app.worker.celery_app.celery_app worker --loglevel=info --concurrency=4" # scale up to 4 workers (hard code for local dev)

  frontend:
    build:
      context: ./frontend

      dockerfile: Dockerfile.dev

    container_name: chess_analytics_frontend

    environment:
      - VITE_API_URL=http://localhost:8000

    ports:
      - "5173:5173"

    volumes:
      - ./frontend:/app
      - /app/node_modules

    depends_on:
      - api

    command:
      - npm
      - run
      - dev
      - "--"
      - "--host"

volumes:
  postgres_data:
